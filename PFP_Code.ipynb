{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkmeAM9CmjqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152dfe26-312c-4681-87e8-42ede6eb6313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 112 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 133 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 143 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 153 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 174 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 184 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 194 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 215 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 225 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 235 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 256 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 266 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 276 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 286 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 296 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 307 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 317 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 327 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 337 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 348 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 358 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 368 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 378 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 389 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 399 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 409 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 419 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 430 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 439 kB 4.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet mrjob==0.7.4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mount my google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "BHYtG586m_B7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab295879-71c5-4fe7-bf13-1199ccdd7fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file empsum.py\n",
        "from mrjob.job import MRJob\n",
        "import re\n",
        "import uuid\n",
        "import heapq as h\n",
        "tempFile = \"/content/gdrive/MyDrive/datasets/mr/Flist.csv\"\n",
        "nDigitId = 5\n",
        "arr={}\n",
        "def func(a):\n",
        "  return a\n",
        "\n",
        "class treeNode:\n",
        "  def __init__(self, nameValue, numOccur, parentNode):\n",
        "        self.name = nameValue\n",
        "        self.count = numOccur\n",
        "        self.nodeLink = None\n",
        "        self.parent = parentNode      #needs to be updated\n",
        "        self.children = {} \n",
        "#increments the count variable with a given amount    \n",
        "  def inc(self, numOccur):\n",
        "        self.count += numOccur\n",
        "#display tree in text. Useful for debugging        \n",
        "  def disp(self, ind=1):\n",
        "        print ('  '*ind, self.name, ' ', self.count)\n",
        "        for child in self.children.values():\n",
        "            child.disp(ind+1)\n",
        "\n",
        "  def updateHeader(nodeToTest, targetNode):   #this version does not use recursion\n",
        "    while (nodeToTest.nodeLink != None):    #Do not use recursion to traverse a linked list!\n",
        "        nodeToTest = nodeToTest.nodeLink\n",
        "    nodeToTest.nodeLink = targetNode\n",
        "\n",
        "  def updateTree(items, inTree, headerTable, count):\n",
        "    if items[0] in inTree.children:#check if orderedItems[0] in retTree.children\n",
        "        inTree.children[items[0]].inc(count) #incrament count\n",
        "    else:   #add items[0] to inTree.children\n",
        "        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n",
        "        if headerTable[items[0]][1] == None: #update header table \n",
        "            headerTable[items[0]][1] = inTree.children[items[0]]\n",
        "        else:\n",
        "            treeNode.updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
        "    if len(items) > 1:#call updateTree() with remaining ordered items\n",
        "        treeNode.updateTree(items[1::], inTree.children[items[0]], headerTable, count)\n",
        "\n",
        "\n",
        "  def createTree(dataSet, minSup=1): #create FP-tree from dataset but don't mine\n",
        "    headerTable = {}\n",
        "    #go over dataSet twice\n",
        "    for trans in dataSet:#first pass counts frequency of occurance\n",
        "        for item in trans:\n",
        "            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n",
        "    for k in list(headerTable):  #remove items not meeting minSup\n",
        "        if headerTable[k] < minSup: \n",
        "            del(headerTable[k])\n",
        "    freqItemSet = set(headerTable.keys())\n",
        "    #print 'freqItemSet: ',freqItemSet\n",
        "    if len(freqItemSet) == 0: return None, None  #if no items meet min support -->get out\n",
        "    for k in headerTable:\n",
        "        headerTable[k] = [headerTable[k], None] #reformat headerTable to use Node link \n",
        "    #print 'headerTable: ',headerTable\n",
        "    retTree = treeNode('Null Set', 1, None) #create tree\n",
        "    for tranSet, count in dataSet.items():  #go through dataset 2nd time\n",
        "        localD = {}\n",
        "        for item in tranSet:  #put transaction items in order\n",
        "            if item in freqItemSet:\n",
        "                localD[item] = headerTable[item][0]\n",
        "        if len(localD) > 0:\n",
        "            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]\n",
        "            treeNode.updateTree(orderedItems, retTree, headerTable, count)#populate tree with ordered freq itemset\n",
        "    return retTree, headerTable #return tree and header table\n",
        "\n",
        "  \n",
        "\n",
        "def createInitSet(dataSet):\n",
        "    retDict = {}\n",
        "    for trans in dataSet:\n",
        "        retDict[frozenset(trans)] = 1\n",
        "    return retDict\n",
        "\n",
        "class EmpSum(MRJob):\n",
        "    def mapper(self, key, line):\n",
        "      for word in line.split(\",\"):\n",
        "        if(word != \"\"):\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def reducer(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "class pfp(MRJob):\n",
        "    def mapper_init(self):\n",
        "        f = open(tempFile, 'r')\n",
        "        lines = f.readlines()\n",
        "        arr = {}\n",
        "        gid=[]\n",
        "        id=[]\n",
        "        for i in range(4):\n",
        "            x=str(uuid.uuid4())\n",
        "            id.append(i)\n",
        "        j=0\n",
        "        for line in lines:\n",
        "          record = line.split(',')\n",
        "          arr[ record[0] ] = int(record[1])\n",
        "          gid.append(record[0])\n",
        "          j=j+1\n",
        "        #print(dno_avgs)\n",
        "        self.inx=None \n",
        "        self.arr = arr\n",
        "        self.gid=gid\n",
        "        self.id=id\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        f = open(\"/content/gdrive/MyDrive/datasets/DataSetAA.csv\", 'r')\n",
        "        lines = f.readlines()\n",
        "        self.a = []\n",
        "        for i in line.split(\",\"):\n",
        "          if(i!=\"\"):\n",
        "            self.a.append(i)\n",
        "        n=len(self.a)\n",
        "        for j in range(n):\n",
        "          self.buf=[]\n",
        "          try:\n",
        "            hashnum = self.id[(self.gid.index(self.a[n-j-1].lower()))%4]\n",
        "            self.inx=self.gid.index(self.a[n-j-1].lower())%4\n",
        "          except:\n",
        "            hashnum = \"\"\n",
        "          if hashnum != \"\":\n",
        "            self.gid[self.inx]= None\n",
        "            for i in range(n-j):\n",
        "              self.buf.append(self.a[i])\n",
        "          #if(len(self.buf)!=0):\n",
        "          yield(hashnum,self.buf)\n",
        "\n",
        "    def reducer(self, word, counts):\n",
        "          yield (\"word\", counts)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    inputFile = \"/content/gdrive/MyDrive/datasets/DataSetAA.csv\"\n",
        "    outFile = tempFile\n",
        "    mr_job = EmpSum(args=[inputFile])\n",
        "    with mr_job.make_runner() as runner:\n",
        "      runner.run()\n",
        "      f = open(outFile, \"w\") \n",
        "      for key, value in mr_job.parse_output(runner.cat_output()):\n",
        "        s = f'{key},{value}'\n",
        "        print(s, file = f)\n",
        "      f.close()\n",
        "\n",
        "    mr_job2 = pfp(args=[inputFile])\n",
        "    with mr_job2.make_runner() as runner:\n",
        "      runner.run()\n",
        "      for key, value in mr_job2.parse_output(runner.cat_output()):\n",
        "        print(key,value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbT0-k99ndbS",
        "outputId": "da66d633-f156-4e6a-f5b5-2c8cbe7ed763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing empsum.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python empsum.py \"/content/gdrive/MyDrive/datasets/DataSetAA.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NT8D_HkMw9K9",
        "outputId": "76bfa7aa-0ca7-468e-d83a-e8cab174f918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs specified for inline runner\n",
            "No configs specified for inline runner\n",
            "\n",
            "Error while reading from /tmp/empsum.root.20221217.174014.581633/step/000/reducer/00000/input:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"empsum.py\", line 151, in <module>\n",
            "    runner.run()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/runner.py\", line 503, in run\n",
            "    self._run()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 161, in _run\n",
            "    self._run_step(step, step_num)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 170, in _run_step\n",
            "    self._run_streaming_step(step, step_num)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 187, in _run_streaming_step\n",
            "    self._run_reducers(step_num, num_reducer_tasks)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 289, in _run_reducers\n",
            "    self._run_multiple(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 130, in _run_multiple\n",
            "    func()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 746, in _run_task\n",
            "    invoke_task(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/inline.py\", line 133, in invoke_task\n",
            "    task.execute()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/job.py\", line 681, in execute\n",
            "    self.run_reducer(self.options.step_num)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/job.py\", line 796, in run_reducer\n",
            "    write_line(k, v)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/job.py\", line 975, in write_line\n",
            "    self.stdout.write(write(key, value))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/protocol.py\", line 106, in write\n",
            "    return self._dumps(key) + b'\\t' + self._dumps(value)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/protocol.py\", line 138, in _dumps\n",
            "    return json.dumps(value).encode('utf_8')\n",
            "  File \"/usr/lib/python3.8/json/__init__.py\", line 231, in dumps\n",
            "    return _default_encoder.encode(obj)\n",
            "  File \"/usr/lib/python3.8/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.8/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/usr/lib/python3.8/json/encoder.py\", line 179, in default\n",
            "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
            "TypeError: Object of type generator is not JSON serializable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class treeNode:\n",
        "  def __init__(self, nameValue, numOccur, parentNode):\n",
        "        self.name = nameValue\n",
        "        self.count = numOccur\n",
        "        self.nodeLink = None\n",
        "        self.parent = parentNode      #needs to be updated\n",
        "        self.children = {} \n",
        "#increments the count variable with a given amount    \n",
        "  def inc(self, numOccur):\n",
        "        self.count += numOccur\n",
        "#display tree in text. Useful for debugging        \n",
        "  def disp(self, ind=1):\n",
        "        print ('  '*ind, self.name, ' ', self.count)\n",
        "        for child in self.children.values():\n",
        "            child.disp(ind+1)\n",
        "\n",
        "  def updateHeader(nodeToTest, targetNode):   #this version does not use recursion\n",
        "    while (nodeToTest.nodeLink != None):    #Do not use recursion to traverse a linked list!\n",
        "        nodeToTest = nodeToTest.nodeLink\n",
        "    nodeToTest.nodeLink = targetNode\n",
        "\n",
        "  def updateTree(items, inTree, headerTable, count):\n",
        "    if items[0] in inTree.children:#check if orderedItems[0] in retTree.children\n",
        "        inTree.children[items[0]].inc(count) #incrament count\n",
        "    else:   #add items[0] to inTree.children\n",
        "        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n",
        "        if headerTable[items[0]][1] == None: #update header table \n",
        "            headerTable[items[0]][1] = inTree.children[items[0]]\n",
        "        else:\n",
        "            treeNode.updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n",
        "    if len(items) > 1:#call updateTree() with remaining ordered items\n",
        "        treeNode.updateTree(items[1::], inTree.children[items[0]], headerTable, count)\n",
        "\n",
        "\n",
        "  def createTree(dataSet, minSup=1): #create FP-tree from dataset but don't mine\n",
        "    headerTable = {}\n",
        "    #go over dataSet twice\n",
        "    for trans in dataSet:#first pass counts frequency of occurance\n",
        "        for item in trans:\n",
        "            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n",
        "    for k in list(headerTable):  #remove items not meeting minSup\n",
        "        if headerTable[k] < minSup: \n",
        "            del(headerTable[k])\n",
        "    freqItemSet = set(headerTable.keys())\n",
        "    #print 'freqItemSet: ',freqItemSet\n",
        "    if len(freqItemSet) == 0: return None, None  #if no items meet min support -->get out\n",
        "    for k in headerTable:\n",
        "        headerTable[k] = [headerTable[k], None] #reformat headerTable to use Node link \n",
        "    #print 'headerTable: ',headerTable\n",
        "    retTree = treeNode('Null Set', 1, None) #create tree\n",
        "    for tranSet, count in dataSet.items():  #go through dataset 2nd time\n",
        "        localD = {}\n",
        "        for item in tranSet:  #put transaction items in order\n",
        "            if item in freqItemSet:\n",
        "                localD[item] = headerTable[item][0]\n",
        "        if len(localD) > 0:\n",
        "            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]\n",
        "            treeNode.updateTree(orderedItems, retTree, headerTable, count)#populate tree with ordered freq itemset\n",
        "    return retTree, headerTable #return tree and header table\n",
        "\n",
        "  \n",
        "\n",
        "def createInitSet(dataSet):\n",
        "    retDict = {}\n",
        "    for trans in dataSet:\n",
        "        retDict[frozenset(trans)] = 1\n",
        "    return retDict\n"
      ],
      "metadata": {
        "id": "0o4-Voe5VXPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO6bPccCXxTk",
        "outputId": "90afb6e5-6fba-47c9-9d12-82ce88cfa3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mrjob in /usr/local/lib/python3.8/dist-packages (0.7.4)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from mrjob) (6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from itertools import chain, combinations\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "gnFJNhj1K3jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FPNode:\n",
        "    def __init__(self, item, count, parent):\n",
        "        self.item = item\n",
        "        self.count = count\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.link = None\n",
        "\n",
        "    def add(self, transaction, item_freq):\n",
        "        if transaction:\n",
        "            item = transaction[0]\n",
        "            child = self.children.get(item)\n",
        "            if child is None:\n",
        "                child = FPNode(item, item_freq[item], self)\n",
        "                self.children[item] = child\n",
        "                if self.link is not None:\n",
        "                    child.link = self.link.children.get(item)\n",
        "                    self.link.children[item] = child\n",
        "                else:\n",
        "                    child.link = None\n",
        "            child.count += 1\n",
        "            child.add(transaction[1:], item_freq)\n",
        "\n",
        "    def prefix_paths(self, item):\n",
        "        paths = []\n",
        "        if self.item == item:\n",
        "            paths.append([self])\n",
        "        for child in self.children.values():\n",
        "            for path in child.prefix_paths(item):\n",
        "                paths.append([self] + path)\n",
        "        return paths\n"
      ],
      "metadata": {
        "id": "yBs88maLK1Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4B32MYwfpFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQbUk_GofpIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EXPTAQHAfpOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file temp.py\n",
        "class node:\n",
        "\n",
        "    def __init__(self, name, freq):\n",
        "        self.name = name\n",
        "        self.freq = freq\n",
        "        self.children=[]\n",
        "        self.child={}\n",
        "        \n",
        "    def disp(self, ind=1):\n",
        "        print ('  '*ind, self.name, '  ', self.freq)\n",
        "        for child in self.child.values():\n",
        "            child.disp(ind+2)\n",
        "\n",
        "\n",
        "class FPNode:\n",
        "      def __init__(self, item, parent, count=1):\n",
        "          self.item = item\n",
        "          self.parent = parent\n",
        "          self.count = count\n",
        "          self.children = []\n",
        "\n",
        "      def add_child(self, child):\n",
        "          self.children.append(child)\n",
        "\n",
        "      def __repr__(self):\n",
        "          return f\"FPNode(item={self.item}, count={self.count})\"\n",
        "\n",
        "class FPTree:\n",
        "    def __init__(self, transactions, min_support=1):\n",
        "        # Create a frequency map of the items in the transactions\n",
        "        item_freq = defaultdict(int)\n",
        "        for transaction in transactions:\n",
        "            for item in transaction:\n",
        "                item_freq[item] += 1\n",
        "\n",
        "        # Filter out items that don't meet the minimum support\n",
        "        items = [item for item, count in item_freq.items() if count >= min_support]\n",
        "\n",
        "        # Sort the items by frequency in decreasing order\n",
        "        items.sort(key=lambda x: item_freq[x], reverse=True)\n",
        "\n",
        "        # Create the FP-tree\n",
        "        self.root = FPNode(\"root\", None, None)\n",
        "        for transaction in transactions:\n",
        "            sorted_items = [item for item in transaction if item in items]\n",
        "            self._add_transaction(sorted_items, self.root, item_freq)\n",
        "\n",
        "    def _add_transaction(self, items, node, item_freq):\n",
        "        # If there are no items left, return\n",
        "        if not items:\n",
        "            return\n",
        "\n",
        "        # Get the first item in the list\n",
        "        item = items[0]\n",
        "\n",
        "        # Try to find a child node for the item\n",
        "        child = next((x for x in node.children if x.item == item), None)\n",
        "\n",
        "        # If a child node doesn't exist, create one\n",
        "        if not child:\n",
        "            child = FPNode(item, node, item_freq[item])\n",
        "            node.add_child(child)\n",
        "\n",
        "        # Increment the count for the item\n",
        "        child.count += 1\n",
        "\n",
        "        # Add the remaining items to the FP-tree\n",
        "        self._add_transaction(items[1:], child, item_freq)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self._repr(self.root, 0)\n",
        "\n",
        "    def _repr(self, node, depth):\n",
        "        result = \"\"\n",
        "        if node.children:\n",
        "            result += \" \" * depth + str(node) + \"\\n\"\n",
        "            for child in node.children:\n",
        "                result += self._repr(child, depth + 1)\n",
        "        else:\n",
        "            result += \" \" * depth + str(node) + \"\\n\"\n",
        "        return result"
      ],
      "metadata": {
        "id": "i_UCc1asfpRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file empsum1.py\n",
        "from mrjob.job import MRJob\n",
        "from temp import FPNode\n",
        "from temp import FPTree\n",
        "from temp import node\n",
        "import numpy as np\n",
        "import re\n",
        "import uuid\n",
        "import json\n",
        "import heapq as h\n",
        "tempFile = \"/content/gdrive/MyDrive/Flist.csv\"\n",
        "nDigitId = 5\n",
        "\n",
        "    \n",
        "def update(n,k):\n",
        "    if k in n.children:\n",
        "        n.child[k].freq+=1;\n",
        "        \n",
        "    else:\n",
        "        n.child[k]=node(k,1)\n",
        "        n.children.append(k)\n",
        "       \n",
        "class EmpSum(MRJob):\n",
        "    def mapper(self, key, line):\n",
        "      for word in line.split(\",\"):\n",
        "        if(word != \"\"):\n",
        "            yield word.lower(), 1\n",
        "\n",
        "    def reducer(self, word, counts):\n",
        "        yield word, sum(counts)\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class pfp(MRJob):\n",
        "    \n",
        "\n",
        "    def mapper_init(self):\n",
        "        f = open(tempFile, 'r')\n",
        "        lines = f.readlines()\n",
        "        arr = {}\n",
        "        gid=[]\n",
        "        id=[]\n",
        "        for i in range(4):\n",
        "            x=str(uuid.uuid4())\n",
        "            id.append(i)\n",
        "        j=0\n",
        "        for line in lines:\n",
        "          record = line.split(',')\n",
        "          arr[ record[0] ] = int(record[1])\n",
        "          gid.append(record[0])\n",
        "          j=j+1\n",
        "        #print(dno_avgs)\n",
        "        self.inx=None \n",
        "        self.arr = arr\n",
        "        self.gid=gid\n",
        "        self.id=id\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        f = open(\"/content/gdrive/MyDrive/DataSetAA.csv\", 'r')\n",
        "        lines = f.readlines()\n",
        "        self.a = []\n",
        "        for i in line.split(\",\"):\n",
        "          if(i!=\"\"):\n",
        "            self.a.append(i)\n",
        "        n=len(self.a)\n",
        "        for j in range(n):\n",
        "          self.buf=[]\n",
        "          try:\n",
        "            hashnum = self.id[(self.gid.index(self.a[n-j-1].lower()))%4]\n",
        "            self.inx=self.gid.index(self.a[n-j-1].lower())%4\n",
        "          except:\n",
        "            hashnum = \"\"\n",
        "          if hashnum != \"\":\n",
        "            self.gid[self.inx]= None\n",
        "            for i in range(n-j):\n",
        "              self.buf.append(self.a[i])\n",
        "          if(len(self.buf)!=0):\n",
        "            yield(hashnum,list(self.buf))\n",
        "    def reducer_init(self):\n",
        "        f = open(tempFile, 'r')\n",
        "        lines = f.readlines()\n",
        "        arr = {}\n",
        "        gid=[]\n",
        "        id=[]\n",
        "        for i in range(4):\n",
        "            id.append(i)\n",
        "        j=0\n",
        "        for line in lines:\n",
        "          record = line.split(',')\n",
        "          arr[ record[0] ] = int(record[1])\n",
        "          gid.append(record[0])\n",
        "          j=j+1\n",
        "        #print(dno_avgs)\n",
        "        self.arr = arr\n",
        "        self.gid=gid\n",
        "        self.id=id\n",
        "    def reducer(self,key,value):\n",
        "      ml=json.dumps(list(value))     \n",
        "      # itr= self.id.index(key)\n",
        "      # x=[]\n",
        "      # for p in range(4):\n",
        "      #   if (itr + p*4)<len(self.arr):\n",
        "      #     x.append(self.gid[itr + p*4])\n",
        "      # null=FPNode(ml,None)\n",
        "      \n",
        "      yield 0, ml\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    inputFile = \"/content/gdrive/MyDrive/DataSetAA.csv\"\n",
        "    outFile = tempFile\n",
        "    mr_job = EmpSum(args=[inputFile])\n",
        "    with mr_job.make_runner() as runner:\n",
        "      runner.run()\n",
        "      f = open(outFile, \"w\") \n",
        "      for key, value in mr_job.parse_output(runner.cat_output()):\n",
        "        s = f'{key},{value}'\n",
        "        print(s, file = f)\n",
        "      f.close()\n",
        "\n",
        "    mr_job2 = pfp(args=[inputFile])\n",
        "    with mr_job2.make_runner() as runner:\n",
        "      runner.run()\n",
        "      for key, value in mr_job2.parse_output(runner.cat_output()):\n",
        "        print(key,value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298a29e3-9692-409a-9442-698c4e165b47",
        "id": "lvAYJoFDVXha"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting empsum1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python empsum.py \"/content/gdrive/MyDrive/datasets/DataSetAA.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCSbVL0BgQdF",
        "outputId": "a2f1721a-82cd-4912-ff42-97643f5d5a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs specified for inline runner\n",
            "No configs specified for inline runner\n",
            "\n",
            "Error while reading from /tmp/empsum.root.20221217.193311.530273/step/000/reducer/00000/input:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"empsum.py\", line 151, in <module>\n",
            "    runner.run()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/runner.py\", line 503, in run\n",
            "    self._run()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 161, in _run\n",
            "    self._run_step(step, step_num)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 170, in _run_step\n",
            "    self._run_streaming_step(step, step_num)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 187, in _run_streaming_step\n",
            "    self._run_reducers(step_num, num_reducer_tasks)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 289, in _run_reducers\n",
            "    self._run_multiple(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 130, in _run_multiple\n",
            "    func()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/sim.py\", line 746, in _run_task\n",
            "    invoke_task(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/inline.py\", line 133, in invoke_task\n",
            "    task.execute()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/job.py\", line 681, in execute\n",
            "    self.run_reducer(self.options.step_num)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/job.py\", line 796, in run_reducer\n",
            "    write_line(k, v)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/job.py\", line 975, in write_line\n",
            "    self.stdout.write(write(key, value))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/protocol.py\", line 106, in write\n",
            "    return self._dumps(key) + b'\\t' + self._dumps(value)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/mrjob/protocol.py\", line 138, in _dumps\n",
            "    return json.dumps(value).encode('utf_8')\n",
            "  File \"/usr/lib/python3.8/json/__init__.py\", line 231, in dumps\n",
            "    return _default_encoder.encode(obj)\n",
            "  File \"/usr/lib/python3.8/json/encoder.py\", line 199, in encode\n",
            "    chunks = self.iterencode(o, _one_shot=True)\n",
            "  File \"/usr/lib/python3.8/json/encoder.py\", line 257, in iterencode\n",
            "    return _iterencode(o, 0)\n",
            "  File \"/usr/lib/python3.8/json/encoder.py\", line 179, in default\n",
            "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
            "TypeError: Object of type generator is not JSON serializable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGOhYK4WgW_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}